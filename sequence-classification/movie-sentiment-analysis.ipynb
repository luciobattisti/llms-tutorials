{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb700a64-46f5-421f-affc-408915de93da",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Original Notebook [here](https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/fine-tuning/ft-example.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e65d3-8c93-4fdd-bb4b-50001abcab3f",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e9b79b-1aa4-422b-8d1a-030edf060874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20077f08-b1a7-42a7-bea0-fade7a585b4d",
   "metadata": {},
   "source": [
    "# Import Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53f54c3-c07f-479e-b40b-2bad12b9c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# Define label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "# Generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48301594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc603b-5cf6-4453-8ab5-a2c4a246ffb8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38a6c8b-a2ca-4cb9-881c-977ad9f9c6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf4e5d1-f948-4851-986c-9f144aef99f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': '. . . or type on a computer keyboard, they\\'d probably give this eponymous film a rating of \"10.\" After all, no elephants are shown being killed during the movie; it is not even implied that any are hurt. To the contrary, the master of ELEPHANT WALK, John Wiley (Peter Finch), complains that he cannot shoot any of the pachyderms--no matter how menacing--without a permit from the government (and his tone suggests such permits are not within the realm of probability). Furthermore, the elements conspire--in the form of an unusual drought and a human cholera epidemic--to leave the Wiley plantation house vulnerable to total destruction by the Elephant People (as the natives dub them) to close the story. If you happen to see the current release EARTH, you\\'ll detect the Elephant People are faring less well today.'}\n",
      "\n",
      "{'label': 1, 'text': \"During 1933 this film had many cuts taken from it because it was very over the top for the story content and the fact that Lily Powers,(Barbara Stanwyck) would do anything to obtain great wealth and power. Lily's father had forced his daughter into prostitution at the age of 14 and she grew up in a steel mill of a town with very poor people and her father ran a speakeasy which brought into his home all kinds of male characters who had their eye on Lily. As the story progresses, Lily meets up with man after man and eventually finds a guy who has everything and is a playboy bank president It is great to see a very young John Wayne, (Jimmy McCoy Jr.) who was only 25 when this picture was produced and Jimmy did not even get to first base with Lily, not even for lunch. A very young George Brent, (Coutland Trenholm) stars along with Barbara Stanwyck and both gave outstanding performances. This is a great film from 1933 which was produced by Darryl F. Zanuck and was locked up in a fault for many years and just recently is being shown on the silver screen. This film is rather mild compared to what we view on the Hollywood screens today, but in 1933 it was very naughty to watch this type of film. Enjoy\"}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for example in dataset[\"train\"]:\n",
    "    print(example)\n",
    "    \n",
    "    if count >= 1:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87456c4",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390bb752-71bb-4777-a913-6a1aca035694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# Add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c72d58-15b9-4d33-bfcf-c6fded0241e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # Extract text\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    # Tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db67bba4-36bf-4873-a22a-4b715de9b9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ed0467-0f79-43a9-a9c3-faca871f9a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': '. . . or type on a computer keyboard, they\\'d probably give this eponymous film a rating of \"10.\" After all, no elephants are shown being killed during the movie; it is not even implied that any are hurt. To the contrary, the master of ELEPHANT WALK, John Wiley (Peter Finch), complains that he cannot shoot any of the pachyderms--no matter how menacing--without a permit from the government (and his tone suggests such permits are not within the realm of probability). Furthermore, the elements conspire--in the form of an unusual drought and a human cholera epidemic--to leave the Wiley plantation house vulnerable to total destruction by the Elephant People (as the natives dub them) to close the story. If you happen to see the current release EARTH, you\\'ll detect the Elephant People are faring less well today.', 'input_ids': [101, 1012, 1012, 1012, 2030, 2828, 2006, 1037, 3274, 9019, 1010, 2027, 1005, 1040, 2763, 2507, 2023, 15248, 2143, 1037, 5790, 1997, 1000, 2184, 1012, 1000, 2044, 2035, 1010, 2053, 16825, 2024, 3491, 2108, 2730, 2076, 1996, 3185, 1025, 2009, 2003, 2025, 2130, 13339, 2008, 2151, 2024, 3480, 1012, 2000, 1996, 10043, 1010, 1996, 3040, 1997, 10777, 3328, 1010, 2198, 18825, 1006, 2848, 16133, 1007, 1010, 17612, 2015, 2008, 2002, 3685, 5607, 2151, 1997, 1996, 14397, 10536, 4063, 5244, 1011, 1011, 2053, 3043, 2129, 24060, 1011, 1011, 2302, 1037, 9146, 2013, 1996, 2231, 1006, 1998, 2010, 4309, 6083, 2107, 14245, 2024, 2025, 2306, 1996, 8391, 1997, 9723, 1007, 1012, 7297, 1010, 1996, 3787, 9530, 13102, 7442, 1011, 1011, 1999, 1996, 2433, 1997, 2019, 5866, 14734, 1998, 1037, 2529, 25916, 16311, 1011, 1011, 2000, 2681, 1996, 18825, 10065, 2160, 8211, 2000, 2561, 6215, 2011, 1996, 10777, 2111, 1006, 2004, 1996, 12493, 12931, 2068, 1007, 2000, 2485, 1996, 2466, 1012, 2065, 2017, 4148, 2000, 2156, 1996, 2783, 2713, 3011, 1010, 2017, 1005, 2222, 11487, 1996, 10777, 2111, 2024, 2521, 2075, 2625, 2092, 2651, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'label': 1, 'text': \"During 1933 this film had many cuts taken from it because it was very over the top for the story content and the fact that Lily Powers,(Barbara Stanwyck) would do anything to obtain great wealth and power. Lily's father had forced his daughter into prostitution at the age of 14 and she grew up in a steel mill of a town with very poor people and her father ran a speakeasy which brought into his home all kinds of male characters who had their eye on Lily. As the story progresses, Lily meets up with man after man and eventually finds a guy who has everything and is a playboy bank president It is great to see a very young John Wayne, (Jimmy McCoy Jr.) who was only 25 when this picture was produced and Jimmy did not even get to first base with Lily, not even for lunch. A very young George Brent, (Coutland Trenholm) stars along with Barbara Stanwyck and both gave outstanding performances. This is a great film from 1933 which was produced by Darryl F. Zanuck and was locked up in a fault for many years and just recently is being shown on the silver screen. This film is rather mild compared to what we view on the Hollywood screens today, but in 1933 it was very naughty to watch this type of film. Enjoy\", 'input_ids': [101, 2076, 4537, 2023, 2143, 2018, 2116, 7659, 2579, 2013, 2009, 2138, 2009, 2001, 2200, 2058, 1996, 2327, 2005, 1996, 2466, 4180, 1998, 1996, 2755, 2008, 7094, 4204, 1010, 1006, 6437, 9761, 18418, 3600, 1007, 2052, 2079, 2505, 2000, 6855, 2307, 7177, 1998, 2373, 1012, 7094, 1005, 1055, 2269, 2018, 3140, 2010, 2684, 2046, 15016, 2012, 1996, 2287, 1997, 2403, 1998, 2016, 3473, 2039, 1999, 1037, 3886, 4971, 1997, 1037, 2237, 2007, 2200, 3532, 2111, 1998, 2014, 2269, 2743, 1037, 3713, 5243, 6508, 2029, 2716, 2046, 2010, 2188, 2035, 7957, 1997, 3287, 3494, 2040, 2018, 2037, 3239, 2006, 7094, 1012, 2004, 1996, 2466, 22901, 1010, 7094, 6010, 2039, 2007, 2158, 2044, 2158, 1998, 2776, 4858, 1037, 3124, 2040, 2038, 2673, 1998, 2003, 1037, 18286, 2924, 2343, 2009, 2003, 2307, 2000, 2156, 1037, 2200, 2402, 2198, 6159, 1010, 1006, 5261, 16075, 3781, 1012, 1007, 2040, 2001, 2069, 2423, 2043, 2023, 3861, 2001, 2550, 1998, 5261, 2106, 2025, 2130, 2131, 2000, 2034, 2918, 2007, 7094, 1010, 2025, 2130, 2005, 6265, 1012, 1037, 2200, 2402, 2577, 12895, 1010, 1006, 2522, 4904, 3122, 29461, 25311, 4747, 2213, 1007, 3340, 2247, 2007, 6437, 9761, 18418, 3600, 1998, 2119, 2435, 5151, 4616, 1012, 2023, 2003, 1037, 2307, 2143, 2013, 4537, 2029, 2001, 2550, 2011, 22821, 1042, 1012, 23564, 11231, 3600, 1998, 2001, 5299, 2039, 1999, 1037, 6346, 2005, 2116, 2086, 1998, 2074, 3728, 2003, 2108, 3491, 2006, 1996, 3165, 3898, 1012, 2023, 2143, 2003, 2738, 10256, 4102, 2000, 2054, 2057, 3193, 2006, 1996, 5365, 12117, 2651, 1010, 2021, 1999, 4537, 2009, 2001, 2200, 20355, 2000, 3422, 2023, 2828, 1997, 2143, 1012, 5959, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for example in tokenized_dataset[\"train\"]:\n",
    "    print(example)\n",
    "    \n",
    "    if count >= 1:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed81c28b-d834-418e-bc0e-d98556a765a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84264a-93fe-47c5-a880-4849b9017a22",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1692a54c-1c8c-4017-9616-a77387775ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, \n",
    "                                          references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d018b-97ed-45ae-b944-b0b87e80abc1",
   "metadata": {},
   "source": [
    "# Apply Untrained Model to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3afaff20-6f22-468d-9d26-caad033bd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "It was good. - Negative\n",
      "Not a fan, don't recommed. - Negative\n",
      "Better than the first one. - Negative\n",
      "This is not worth watching even once. - Negative\n",
      "This one is a pass. - Negative\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # Compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # Convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2323b-28b7-443e-9f30-2f5a101cea03",
   "metadata": {},
   "source": [
    "# Fine tuning with LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde7468a-2496-4f0e-85dd-404930b71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", # Sequence classification\n",
    "    r=4, # Intrinsic rank of trainable weight matrix\n",
    "    lora_alpha=32, # This is like a learning rate\n",
    "    lora_dropout=0.01, # Probablity of dropout\n",
    "    target_modules = ['q_lin'] # We apply lora to query layer only\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c200a7-6da9-4b14-9377-c2a508956f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9306847223789819\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9991943-dc0f-4b80-82e0-196288f15d7f",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3663a9-beac-442b-a3e7-1824f4d92f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # Size of optimization step \n",
    "batch_size = 4 # Number of examples processed per optimziation step\n",
    "num_epochs = 5 # Number of times model runs through training data\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875072b-76f0-4390-bc5e-0a1a62053dbe",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73ad7f48-97ae-4cd6-af24-78c45a3cf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 2:19:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355744</td>\n",
       "      <td>{'accuracy': 0.878}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.593929</td>\n",
       "      <td>{'accuracy': 0.862}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.568574</td>\n",
       "      <td>{'accuracy': 0.881}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.664931</td>\n",
       "      <td>{'accuracy': 0.881}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.688521</td>\n",
       "      <td>{'accuracy': 0.88}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.25873106002807617, metrics={'train_runtime': 8348.8409, 'train_samples_per_second': 0.599, 'train_steps_per_second': 0.15, 'total_flos': 556608875967840.0, 'train_loss': 0.25873106002807617, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, # Our peft model\n",
    "    args=training_args, # Hyperparameters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # Training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # Validation data\n",
    "    tokenizer=tokenizer, # Define tokenizer\n",
    "    data_collator=data_collator, # This will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics, # Evaluates model using compute_metrics() function from before\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24077449-d81f-4eb7-8ab6-618d5029fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "It was good. - Positive\n",
      "Not a fan, don't recommed. - Negative\n",
      "Better than the first one. - Positive\n",
      "This is not worth watching even once. - Positive\n",
      "This one is a pass. - Positive\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\") # Moving to cpu\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\") # Moving to cpu\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e9351-1ee0-4fdf-834b-6dda033621c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
