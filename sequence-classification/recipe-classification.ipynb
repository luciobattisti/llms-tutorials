{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4e65d3-8c93-4fdd-bb4b-50001abcab3f",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e9b79b-1aa4-422b-8d1a-030edf060874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, load_from_disk\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20077f08-b1a7-42a7-bea0-fade7a585b4d",
   "metadata": {},
   "source": [
    "# Import Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53f54c3-c07f-479e-b40b-2bad12b9c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "# Define label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "# Generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48301594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc603b-5cf6-4453-8ab5-a2c4a246ffb8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38a6c8b-a2ca-4cb9-881c-977ad9f9c6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_from_disk(\"../data-manipulation/recipe-classification-dataset\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf4e5d1-f948-4851-986c-9f144aef99f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,tuna pasta \\nsat,lemon pasta \\nsun,lentil salad \\n,\\nIngredient,Quantity\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nlinguine,1.00 lb\\nolive,12.00 oz\\nmushroom,14.00 oz\\npenne,1.00 lb\\ncream,8.00 oz\\nlentils,8.00 oz\\nrocket,5.00 oz\\ntomato,5 items\\nlemon,1 items\\nchives,4 items\\naubergine,1 items\\npepper,1 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nmon,risotto ai funghi\\ntue,pepper pasta\\nwed,cous cous\\nthu,tuna pasta \\nfri,panini paffuti\\n,\\nIngredient,Quantity\\nrice,8.00 oz\\nmushroom,8.00 oz\\nbutter,8.00 oz\\npenne,1.00 lb\\nsauce,30.00 oz\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,4.00 tbs\\nolive,28.00 oz\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nlinguine,1.00 lb\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\nonion,3 items\\nsausage,4 items\\npepper,4 items\\ntomato,11 items\\ncourgette,2 items\\naubergine,1 items\\nbun,4 items\\nprovolone,4 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,pepper pasta\\nsat,pesto pasta\\nsun,tuna pasta \\n,\\nIngredient,Quantity\\npenne,1.00 lb\\nsauce,30.00 oz\\nlinguine,2.00 lb\\npesto,9.00 oz\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nolive,12.00 oz\\npepper,2 items\\nonion,1 items\\ntomato,5 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,cous cous\\nthu,tuna pasta \\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,4.00 tbs\\nolive,20.00 oz\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nlinguine,1.00 lb\\ntomato,9 items\\ncourgette,2 items\\npepper,2 items\\nonion,1 items\\naubergine,1 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': \"Day,Recipe, \\nmon,pasta all'amatriciana\\ntue,fish and rice\\nwed,asparagus pasta\\nthu,rice and beans \\n,\\nIngredient,Quantity\\npancetta,8.00 oz\\nspaghetti,2.00 lb\\nsauce,30.00 oz\\ntuna_fresh,12.00 oz\\nrice,14.00 oz\\npea,18.00 oz\\nolive_oil,4.00 tbs\\nasparagus,10.00 oz\\nkidney_bean,14.00 oz\\nparsley,2.00 oz\\nonion,3 items\\ncarrot,2 items\\nlemon,1 items\\nprovolone,6 items\\ncourgette,1 items\\ntomato,5 items\\n\"}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,classic pasta\\nthu,cous cous\\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\npenne,1.00 lb\\nsauce,30.00 oz\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,4.00 tbs\\nolive,8.00 oz\\naubergine,2 items\\ncourgette,4 items\\nonion,2 items\\ntomato,4 items\\npepper,2 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nmon,broccoli pasta\\ntue,chili \\nwed,panini paffuti\\nsun,pesto pasta\\n,\\nIngredient,Quantity\\npenne,1.00 lb\\nbroccoli,10.00 oz\\nvegetable_stock,2.00 oz\\nbeef,1.00 lb\\nsauce,30.00 oz\\ntomato_puree,8.00 oz\\nkidney_bean,14.00 oz\\ncheddar_cheese,1.00 cup\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\nlinguine,1.00 lb\\npesto,9.00 oz\\nonion,2 items\\ngarlic,1 items\\npepper,1 items\\nchips,1 items\\nsoured_cream,1 items\\nbun,4 items\\ntomato,2 items\\nprovolone,4 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,cous cous\\nwed,fish and rice\\nthu,mushroom pasta\\nfri,panini paffuti\\n,\\nIngredient,Quantity\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,8.00 tbs\\nolive,8.00 oz\\ntuna_fresh,12.00 oz\\nrice,6.00 oz\\npea,4.00 oz\\nmushroom,8.00 oz\\nfettuccine,1.00 lb\\nsauce,30.00 oz\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\ntomato,6 items\\ncourgette,2 items\\npepper,2 items\\nonion,2 items\\naubergine,1 items\\ncarrot,2 items\\nlemon,1 items\\nbun,4 items\\nprovolone,4 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,pepper pasta\\nthu,cauliflower pasta \\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\npenne,1.00 lb and 1 items\\nsauce,30.00 oz\\npepper,3 items\\nonion,2 items\\ncauliflower,1 items\\nolive,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,pasta con fagioli \\nsun,penne amore \\n,\\nIngredient,Quantity\\nrigatoni,1.00 lb\\nsauce,30.00 oz\\nkidney_bean,14.00 oz\\npenne,1.00 lb\\nham,8.00 oz\\ncream,8.00 oz\\npea,8.00 oz\\nonion,1 items\\npepper,1 items\\nchives,3 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,tuna salad and eggs\\nthu,bikini\\nfri,panini paffuti\\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,16.00 oz\\nlettuce,8.00 oz\\nmaize,11.00 oz\\ntuna_tinned,10.00 oz\\nham,8.00 oz\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\ntomato,6 items\\negg,5 items\\nbread,6 items\\nbun,4 items\\nprovolone,4 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nmon,tuna pasta \\ntue,pollo e patate\\nsat,bagels\\nsun,mushroom pasta\\n,\\nIngredient,Quantity\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nlinguine,1.00 lb\\nolive,12.00 oz\\nchicken,1.00 lb\\nbreadcrumbs,1.00 lb\\nmushroom,8.00 oz\\nfettuccine,1.00 lb\\nsauce,30.00 oz\\ntomato,5 items\\npotato,5 items\\noil,1 items\\negg,4 items\\nbagel,4 items\\ncream_cheese,1 items\\nsmoked_salmon,1 items\\nham,1 items\\ncheddar_cheese,4 items\\nonion,1 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nwed,asparagus pasta\\nthu,summer salad\\n,\\nIngredient,Quantity\\nspaghetti,1.00 lb\\nasparagus,10.00 oz\\nrice,12.00 oz\\ntuna_tinned,16.00 oz\\nolive,8.00 oz\\nprovolone,6 items\\nonion,1 items\\ncourgette,1 items\\ntomato,5 items\\negg,4 items\\nmaize,8 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nwed,pizza\\nthu,pasta con fagioli \\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\nrigatoni,1.00 lb\\nsauce,30.00 oz\\nkidney_bean,14.00 oz\\nonion,1 items\\npepper,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,pepper pasta\\nsat,pesto pasta\\nsun,pepper pasta\\n,\\nIngredient,Quantity\\npenne,2.00 lb\\nsauce,60.00 oz\\nlinguine,1.00 lb\\npesto,9.00 oz\\npepper,4 items\\nonion,2 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,fish and rice\\nthu,pepper pasta\\nfri,panini paffuti\\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\ntuna_fresh,12.00 oz\\nrice,6.00 oz\\npea,4.00 oz\\nolive_oil,4.00 tbs\\npenne,1.00 lb\\nsauce,30.00 oz\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\ncarrot,2 items\\nlemon,1 items\\npepper,2 items\\nonion,1 items\\nbun,4 items\\ntomato,2 items\\nprovolone,4 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,asparagus pasta\\nthu,mozzarella pasta\\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\nspaghetti,2.00 lb\\nasparagus,10.00 oz\\nolive,8.00 oz\\nmozzarella_cheese,12.00 oz\\nbasil,4.00 oz\\nprovolone,6 items\\nonion,1 items\\ncourgette,1 items\\ntomato,5 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,tuna pasta \\nthu,rice salad \\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\ntuna_tinned,10.00 oz\\nbasil,5.00 oz\\nlinguine,1.00 lb\\nolive,12.00 oz\\nrice,8.00 oz\\nmozzarella_cheese,1.00 lb\\ncherry_tomatoes,8.00 oz\\nvegetable_stock,2.00 oz\\nspinach,4.00 oz\\ntomato,5 items\\ncourgette,2 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,fish and rice\\nsun,classic pasta\\n,\\nIngredient,Quantity\\ntuna_fresh,12.00 oz\\nrice,6.00 oz\\npea,4.00 oz\\nolive_oil,4.00 tbs\\npenne,1.00 lb\\nsauce,30.00 oz\\ncarrot,2 items\\nlemon,1 items\\naubergine,1 items\\ncourgette,2 items\\nonion,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,mushroom pasta\\nsun,classic pasta\\n,\\nIngredient,Quantity\\nmushroom,8.00 oz\\nfettuccine,1.00 lb\\nsauce,60.00 oz\\npenne,1.00 lb\\nonion,2 items\\naubergine,1 items\\ncourgette,2 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\ntue,pizza\\nwed,tuna pasta \\nthu,pasta con fagioli \\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nlinguine,1.00 lb\\nolive,12.00 oz\\nrigatoni,1.00 lb\\nsauce,30.00 oz\\nkidney_bean,14.00 oz\\ntomato,5 items\\nonion,1 items\\npepper,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': \"Day,Recipe, \\nmon,asparagus pasta\\nsun,pasta all'amatriciana\\n,\\nIngredient,Quantity\\nspaghetti,2.00 lb\\nasparagus,10.00 oz\\npancetta,8.00 oz\\nsauce,30.00 oz\\nprovolone,6 items\\nonion,2 items\\ncourgette,1 items\\n\"}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,broccoli pasta\\nsun,cous cous\\n,\\nIngredient,Quantity\\npenne,1.00 lb\\nbroccoli,10.00 oz\\nvegetable_stock,2.00 oz\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,4.00 tbs\\nolive,8.00 oz\\nonion,2 items\\ntomato,4 items\\ncourgette,2 items\\npepper,2 items\\naubergine,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,asparagus pasta\\nsat,pasta alla carbonara\\nsun,rice and beans \\n,\\nIngredient,Quantity\\nspaghetti,2.00 lb\\nasparagus,10.00 oz\\npancetta,8.00 oz\\nmilk,1.00 cup\\nkidney_bean,14.00 oz\\nrice,8.00 oz\\npea,14.00 oz\\nparsley,2.00 oz\\nprovolone,6 items\\nonion,2 items\\ncourgette,1 items\\negg,6 items\\ntomato,5 items\\n'}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nmon,pepper pasta\\ntue,panini paffuti\\nsat,quesadilla fiesta\\nsun,mozzarella pasta\\n,\\nIngredient,Quantity\\npenne,1.00 lb\\nsauce,30.00 oz\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\nbeef_sirloin,12.00 oz\\ncheese,12.00 oz\\npinto_bean,12.00 oz\\nsoured_cream,12.00 oz\\nspaghetti,1.00 lb\\nolive,8.00 oz\\nmozzarella_cheese,12.00 oz\\nbasil,4.00 oz\\npepper,2 items\\nonion,2 items\\nbun,4 items\\ntomato,10 items\\nprovolone,4 items\\ntortilla_flour,8 items\\nsalsa,1 items\\navocado,3 items\\nseasoning,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,lentil salad \\nsat,pasta alla carbonara\\nsun,cous cous\\n,\\nIngredient,Quantity\\nlentils,8.00 oz\\nrocket,5.00 oz\\npancetta,8.00 oz\\nmilk,1.00 cup\\nspaghetti,1.00 lb\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,4.00 tbs\\nolive,8.00 oz\\naubergine,2 items\\npepper,3 items\\negg,6 items\\ntomato,4 items\\ncourgette,2 items\\nonion,1 items\\n'}\n",
      "\n",
      "{'label': 1, 'text': \"Day,Recipe, \\nmon,broccoli pasta\\nsun,pasta all'amatriciana\\n,\\nIngredient,Quantity\\npenne,1.00 lb\\nbroccoli,10.00 oz\\nvegetable_stock,2.00 oz\\npancetta,8.00 oz\\nspaghetti,1.00 lb\\nsauce,30.00 oz\\nonion,2 items\\n\"}\n",
      "\n",
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,cauliflower pasta \\nsat,fish and rice\\nsun,mushroom pasta\\n,\\nIngredient,Quantity\\ntuna_fresh,12.00 oz\\nrice,6.00 oz\\npea,4.00 oz\\nolive_oil,4.00 tbs\\nmushroom,8.00 oz\\nfettuccine,1.00 lb\\nsauce,30.00 oz\\ncauliflower,1 items\\nonion,2 items\\nolive,1 items\\npenne,1 items\\npepper,1 items\\ncarrot,2 items\\nlemon,1 items\\n'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for example in dataset[\"validation\"]:\n",
    "    print(example)\n",
    "    \n",
    "    if count >= 28:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87456c4",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390bb752-71bb-4777-a913-6a1aca035694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# Add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c72d58-15b9-4d33-bfcf-c6fded0241e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # Extract text\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    # Tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db67bba4-36bf-4873-a22a-4b715de9b9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f19ad575ef343c6ab1b5700443db755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0dd6c5d63844e3a59d45349073fd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ed0467-0f79-43a9-a9c3-faca871f9a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': 'Day,Recipe, \\nmon,broccoli pasta\\nsun,caprese pasta\\n,\\nIngredient,Quantity\\npenne,2.00 lb\\nbroccoli,10.00 oz\\nvegetable_stock,2.00 oz\\ncherry_tomatoes,1.00 lb\\nolive,14.00 oz\\nonion,1 items\\nmozzarella_cheese,1 items\\nbasil,1 items\\n', 'input_ids': [101, 2154, 1010, 17974, 1010, 12256, 1010, 22953, 21408, 3669, 24857, 3103, 1010, 6178, 6072, 2063, 24857, 1010, 21774, 1010, 11712, 9502, 2063, 1010, 1016, 1012, 4002, 6053, 22953, 21408, 3669, 1010, 2184, 1012, 4002, 11472, 15415, 1035, 4518, 1010, 1016, 1012, 4002, 11472, 9115, 1035, 12851, 1010, 1015, 1012, 4002, 6053, 9724, 1010, 2403, 1012, 4002, 11472, 20949, 1010, 1015, 5167, 9587, 20715, 21835, 1035, 8808, 1010, 1015, 5167, 14732, 1010, 1015, 5167, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'label': 0, 'text': 'Day,Recipe, \\nmon,panini paffuti\\ntue,uova al sugo\\nwed,pasta con fagioli \\nthu,mushroom pasta\\n,\\nIngredient,Quantity\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\nsauce,68.00 oz\\nchives,12.00 oz\\nrigatoni,1.00 lb\\nkidney_bean,14.00 oz\\nmushroom,8.00 oz\\nfettuccine,1.00 lb\\nbun,4 items\\ntomato,2 items\\nprovolone,8 items\\negg,6 items\\nonion,2 items\\npepper,1 items\\n', 'input_ids': [101, 2154, 1010, 17974, 1010, 12256, 1010, 6090, 5498, 6643, 4246, 21823, 10722, 2063, 1010, 1057, 7103, 2632, 10514, 3995, 21981, 1010, 24857, 9530, 6904, 11411, 3669, 16215, 2226, 1010, 18565, 24857, 1010, 21774, 1010, 11712, 4013, 11020, 17922, 9284, 1010, 1022, 1012, 4002, 11472, 6714, 6776, 1010, 1022, 1012, 4002, 11472, 12901, 1010, 6273, 1012, 4002, 11472, 9610, 6961, 1010, 2260, 1012, 4002, 11472, 17557, 2669, 2072, 1010, 1015, 1012, 4002, 6053, 14234, 1035, 14068, 1010, 2403, 1012, 4002, 11472, 18565, 1010, 1022, 1012, 4002, 11472, 10768, 4779, 16835, 2638, 1010, 1015, 1012, 4002, 6053, 21122, 1010, 1018, 5167, 20856, 1010, 1016, 5167, 4013, 6767, 27165, 1010, 1022, 5167, 8288, 1010, 1020, 5167, 20949, 1010, 1016, 5167, 11565, 1010, 1015, 5167, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for example in tokenized_dataset[\"train\"]:\n",
    "    print(example)\n",
    "    \n",
    "    if count >= 1:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed81c28b-d834-418e-bc0e-d98556a765a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84264a-93fe-47c5-a880-4849b9017a22",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1692a54c-1c8c-4017-9616-a77387775ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, \n",
    "                                          references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d018b-97ed-45ae-b944-b0b87e80abc1",
   "metadata": {},
   "source": [
    "# Apply Untrained Model to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3afaff20-6f22-468d-9d26-caad033bd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "Day,Recipe, mon,pepper pastasat,panini paffutis - Negative\n",
      "Day,Recipe, tue,pizzawed,classic pastathu,fish  - Negative\n",
      "Day,Recipe, mon,pasta con fagioli sun,penne amor - Negative\n",
      "Day,Recipe, mon,risotto ai funghitue,pepper past - Negative\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\n",
    "    \"Day,Recipe, \\nmon,pepper pasta\\nsat,panini paffuti\\nsun,lentil salad \\n,\\nIngredient,Quantity\\npenne,1.00 lb\\nsauce,30.00 oz\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\nlentils,8.00 oz\\nrocket,5.00 oz\\npepper,3 items\\nonion,1 items\\nbun,4 items\\ntomato,2 items\\nprovolone,4 items\\naubergine,1 items\\n\", \n",
    "    \"Day,Recipe, \\ntue,pizza\\nwed,classic pasta\\nthu,fish and rice\\nfri,panini paffuti\\n,\\nIngredient,Quantity\\npizza,1.00 lb\\npotato_wedges,8.00 oz\\npenne,1.00 lb\\nsauce,30.00 oz\\ntuna_fresh,12.00 oz\\nrice,6.00 oz\\npea,4.00 oz\\nolive_oil,4.00 tbs\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\naubergine,1 items\\ncourgette,2 items\\nonion,1 items\\ncarrot,2 items\\nlemon,1 items\\nbun,4 items\\ntomato,2 items\\nprovolone,4 items\\n\", \n",
    "    \"Day,Recipe, \\nmon,pasta con fagioli \\nsun,penne amore \\n,\\nIngredient,Quantity\\nrigatoni,1.00 lb\\nsauce,30.00 oz\\nkidney_bean,14.00 oz\\npenne,1.00 lb\\nham,8.00 oz\\ncream,8.00 oz\\npea,8.00 oz\\nonion,1 items\\npepper,1 items\\nchives,3 items\\n\", \n",
    "    \"Day,Recipe, \\nmon,risotto ai funghi\\ntue,pepper pasta\\nwed,cous cous\\nthu,tuna pasta \\nfri,panini paffuti\\n,\\nIngredient,Quantity\\nrice,8.00 oz\\nmushroom,8.00 oz\\nbutter,8.00 oz\\npenne,1.00 lb\\nsauce,30.00 oz\\ncouscous,1.00 lb\\ncumin,1.00 tbs\\nturmeric,1.00 tbs\\nolive_oil,4.00 tbs\\nolive,28.00 oz\\ntuna_tinned,10.00 oz\\nbasil,3.00 oz\\nlinguine,1.00 lb\\nprosciutto,8.00 oz\\nspinach,8.00 oz\\nonion,3 items\\nsausage,4 items\\npepper,4 items\\ntomato,11 items\\ncourgette,2 items\\naubergine,1 items\\nbun,4 items\\nprovolone,4 items\\n'\",\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # Compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # Convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text[0:50].replace(\"\\n\", \"\") + \" - \" +id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2323b-28b7-443e-9f30-2f5a101cea03",
   "metadata": {},
   "source": [
    "# Fine tuning with LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde7468a-2496-4f0e-85dd-404930b71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", # Sequence classification\n",
    "    r=4, # Intrinsic rank of trainable weight matrix\n",
    "    lora_alpha=32, # This is like a learning rate\n",
    "    lora_dropout=0.01, # Probablity of dropout\n",
    "    target_modules = ['q_lin'] # We apply lora to query layer only\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c200a7-6da9-4b14-9377-c2a508956f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9306847223789819\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9991943-dc0f-4b80-82e0-196288f15d7f",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3663a9-beac-442b-a3e7-1824f4d92f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # Size of optimization step \n",
    "batch_size = 5 # Number of examples processed per optimziation step\n",
    "num_epochs = 20 # Number of times model runs through training data\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-recipe-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875072b-76f0-4390-bc5e-0a1a62053dbe",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ad7f48-97ae-4cd6-af24-78c45a3cf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 05:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>{'accuracy': 0.5714285714285714}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.624799</td>\n",
       "      <td>{'accuracy': 0.5714285714285714}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.605968</td>\n",
       "      <td>{'accuracy': 0.5714285714285714}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.294615</td>\n",
       "      <td>{'accuracy': 0.9285714285714286}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.049015</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.010376</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.051593</td>\n",
       "      <td>{'accuracy': 0.9642857142857143}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.444209</td>\n",
       "      <td>{'accuracy': 0.8928571428571429}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325855</td>\n",
       "      <td>{'accuracy': 0.8928571428571429}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173616</td>\n",
       "      <td>{'accuracy': 0.9285714285714286}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>{'accuracy': 0.9642857142857143}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>{'accuracy': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.14541549682617189, metrics={'train_runtime': 316.2695, 'train_samples_per_second': 1.771, 'train_steps_per_second': 0.379, 'total_flos': 20965035746184.0, 'train_loss': 0.14541549682617189, 'epoch': 20.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, # Our peft model\n",
    "    args=training_args, # Hyperparameters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # Training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # Validation data\n",
    "    tokenizer=tokenizer, # Define tokenizer\n",
    "    data_collator=data_collator, # This will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics, # Evaluates model using compute_metrics() function from before\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24077449-d81f-4eb7-8ab6-618d5029fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "Day,Recipe, mon,pepper pastasat,panini paffutis - Positive\n",
      "Day,Recipe, tue,pizzawed,classic pastathu,fish  - Negative\n",
      "Day,Recipe, mon,pasta con fagioli sun,penne amor - Positive\n",
      "Day,Recipe, mon,risotto ai funghitue,pepper past - Negative\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\") # Moving to cpu\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\") # Moving to cpu\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text[:50].replace(\"\\n\", \"\") + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e9351-1ee0-4fdf-834b-6dda033621c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
